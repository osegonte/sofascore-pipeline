#!/bin/bash

# Stage 6: Complete ML Model Development Setup
# Creates production-ready ML pipeline for goal prediction

set -e

echo "ðŸš€ SofaScore Pipeline - Stage 6 ML Model Development"
echo "===================================================="
echo ""
echo "Creating complete ML pipeline for football goal prediction..."
echo ""

# Create ML models directory structure
echo "ðŸ“ Creating directory structure..."
mkdir -p src/ml_models/{training,evaluation,prediction,utils}
mkdir -p config/ml_models
mkdir -p data/models/{saved_models,experiments,evaluation}
mkdir -p tests/ml_models

# Create main ML models __init__.py
cat > src/ml_models/__init__.py << 'EOF'
"""
Stage 6: ML Model Development for Goal Prediction

Complete machine learning pipeline for predicting football goals
in 1, 5, and 15 minute windows during matches.
"""

__version__ = "1.0.0"
__author__ = "SofaScore Pipeline Team"

from .training.pipeline import MLTrainingPipeline, TrainingConfig
from .evaluation.framework import ModelEvaluator
from .prediction.api import PredictionEngine, ModelManager, MatchState

__all__ = [
    'MLTrainingPipeline',
    'TrainingConfig', 
    'ModelEvaluator',
    'PredictionEngine',
    'ModelManager',
    'MatchState'
]
EOF

# Create training pipeline configuration
cat > config/ml_models/training_config.json << 'EOF'
{
  "data_processing": {
    "test_size": 0.2,
    "validation_size": 0.2,
    "random_state": 42,
    "stratify": true,
    "handle_imbalance": true
  },
  "feature_engineering": {
    "enable_scaling": true,
    "scaling_method": "standard",
    "enable_feature_selection": true,
    "max_features": 15,
    "selection_methods": ["variance", "mutual_info"],
    "variance_threshold": 0.01
  },
  "models": {
    "xgboost": {
      "enabled": true,
      "base_params": {
        "random_state": 42,
        "eval_metric": "logloss",
        "use_label_encoder": false
      },
      "param_grid": {
        "n_estimators": [100, 200, 300],
        "max_depth": [3, 6, 9],
        "learning_rate": [0.01, 0.1, 0.2],
        "subsample": [0.8, 0.9, 1.0]
      }
    },
    "random_forest": {
      "enabled": true,
      "base_params": {
        "random_state": 42,
        "n_jobs": -1
      },
      "param_grid": {
        "n_estimators": [100, 200, 300],
        "max_depth": [5, 10, 15, null],
        "min_samples_split": [2, 5, 10],
        "min_samples_leaf": [1, 2, 4]
      }
    },
    "logistic_regression": {
      "enabled": true,
      "base_params": {
        "random_state": 42,
        "max_iter": 1000
      },
      "param_grid": {
        "C": [0.1, 1.0, 10.0],
        "penalty": ["l1", "l2"],
        "solver": ["liblinear", "lbfgs"]
      }
    }
  },
  "hyperparameter_tuning": {
    "method": "grid_search",
    "cv_folds": 5,
    "scoring": "f1",
    "n_jobs": -1
  },
  "targets": [
    "goal_next_1min_any",
    "goal_next_5min_any", 
    "goal_next_15min_any"
  ],
  "model_storage": {
    "save_path": "data/models/saved_models",
    "versioning": true,
    "save_preprocessing": true
  }
}
EOF

# Create evaluation configuration
cat > config/ml_models/evaluation_config.json << 'EOF'
{
  "metrics": {
    "classification": [
      "accuracy",
      "precision", 
      "recall",
      "f1",
      "roc_auc",
      "log_loss"
    ],
    "custom": [
      "precision_at_k",
      "recall_at_k",
      "calibration_error"
    ]
  },
  "visualization": {
    "create_plots": true,
    "plot_types": [
      "confusion_matrix",
      "roc_curve", 
      "precision_recall_curve",
      "feature_importance",
      "calibration_plot"
    ],
    "save_plots": true,
    "plot_format": "png",
    "dpi": 300
  },
  "reporting": {
    "generate_html_report": true,
    "include_model_comparison": true,
    "include_feature_analysis": true,
    "export_to_csv": true
  },
  "cross_validation": {
    "enabled": true,
    "folds": 5,
    "shuffle": true,
    "stratified": true
  },
  "output_paths": {
    "reports": "data/models/evaluation/reports",
    "plots": "data/models/evaluation/plots",
    "comparisons": "data/models/evaluation/comparisons"
  }
}
EOF

# Create prediction API configuration  
cat > config/ml_models/prediction_config.json << 'EOF'
{
  "api": {
    "host": "0.0.0.0",
    "port": 8001,
    "title": "Football Goal Prediction API",
    "description": "Real-time goal prediction for football matches",
    "version": "1.0.0"
  },
  "models": {
    "load_best_only": true,
    "max_models_per_target": 3,
    "ensemble_method": "weighted_average",
    "confidence_threshold": 0.6
  },
  "prediction": {
    "default_timeframes": [1, 5, 15],
    "include_confidence": true,
    "include_feature_importance": true,
    "cache_predictions": false
  },
  "performance": {
    "request_timeout": 30,
    "max_concurrent_requests": 100,
    "enable_metrics": true
  },
  "cors": {
    "allow_origins": ["*"],
    "allow_methods": ["GET", "POST"],
    "allow_headers": ["*"]
  }
}
EOF

echo "âœ… Configuration files created"

# Create enhanced requirements for ML
cat > requirements_ml.txt << 'EOF'
# Stage 6: ML Model Development Requirements

# Core ML Libraries
pandas>=1.5.0
numpy>=1.24.0
scikit-learn>=1.3.0
xgboost>=1.7.0

# Data Processing & Analysis
scipy>=1.10.0
joblib>=1.3.0

# Visualization
matplotlib>=3.6.0
seaborn>=0.12.0
plotly>=5.17.0

# API Framework
fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.0.0

# Utilities
click>=8.1.0
tqdm>=4.64.0
python-dotenv>=1.0.0
asyncio-throttle>=1.0.0

# Optional Advanced Features
optuna>=3.0.0  # For hyperparameter optimization
shap>=0.42.0   # For model interpretability
mlflow>=2.0.0  # For experiment tracking (optional)

# Development & Testing
pytest>=7.0.0
pytest-asyncio>=0.21.0
pytest-cov>=4.0.0
black>=23.0.0
isort>=5.12.0
flake8>=6.0.0
EOF

echo "âœ… ML requirements created"

# Create main CLI commands integration
cat > src/ml_models/cli_commands.py << 'EOF'
"""
CLI Commands for Stage 6 ML Model Development.
Comprehensive goal prediction pipeline.
"""

import asyncio
import sys
import json
from pathlib import Path
from typing import List, Optional, Dict, Any
import pandas as pd
import numpy as np
from datetime import datetime

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from .training.pipeline import MLTrainingPipeline, TrainingConfig
    from .evaluation.framework import ModelEvaluator
    from .prediction.api import PredictionEngine, ModelManager, MatchState, run_api_server
    IMPLEMENTATIONS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ML implementations not fully available: {e}")
    IMPLEMENTATIONS_AVAILABLE = False

class MLCommandRunner:
    """Complete ML command runner for Stage 6."""
    
    def __init__(self):
        self.training_pipeline = None
        self.evaluator = None
        self.prediction_engine = None
        self.config_path = Path("config/ml_models")
    
    def load_config(self, config_name: str) -> Dict[str, Any]:
        """Load configuration file."""
        config_file = self.config_path / f"{config_name}.json"
        if config_file.exists():
            with open(config_file) as f:
                return json.load(f)
        return {}
    
    async def run_training(self, training_data: str, target: Optional[str] = None, 
                          models: Optional[List[str]] = None) -> bool:
        """Run comprehensive ML model training."""
        print(f"ðŸŽ¯ Starting Stage 6 ML Training Pipeline...")
        print(f"   Training Data: {training_data}")
        print(f"   Target: {target or 'all targets'}")
        print(f"   Models: {models or 'all models'}")
        
        if not IMPLEMENTATIONS_AVAILABLE:
            print("âŒ Full ML implementations not available. Install requirements:")
            print("   pip install -r requirements_ml.txt")
            return False
        
        try:
            # Load training configuration
            training_config = self.load_config("training_config")
            config = TrainingConfig.from_dict(training_config)
            
            # Initialize training pipeline
            self.training_pipeline = MLTrainingPipeline(config)
            
            # Load and validate training data
            if not Path(training_data).exists():
                print(f"âŒ Training data file not found: {training_data}")
                return False
            
            data = self.training_pipeline.load_training_data(training_data)
            print(f"âœ… Loaded {len(data)} training examples with {len(data.columns)} features")
            
            # Display data summary
            print(f"\nðŸ“Š Training Data Summary:")
            for col in ['goal_next_1min_any', 'goal_next_5min_any', 'goal_next_15min_any']:
                if col in data.columns:
                    positive_count = data[col].sum()
                    total_count = len(data)
                    print(f"   {col}: {positive_count}/{total_count} ({positive_count/total_count*100:.1f}% positive)")
            
            # Train models
            if target and target in data.columns:
                # Train models for specific target
                print(f"\nðŸ”„ Training models for target: {target}")
                target_results = self.training_pipeline.train_all_models(target, models)
                
                print(f"\nðŸ“Š Training Results for {target}:")
                for model_name, result in target_results.items():
                    print(f"   {model_name}:")
                    print(f"     F1 Score: {result.test_f1:.3f}")
                    print(f"     Accuracy: {result.test_accuracy:.3f}")
                    print(f"     AUC: {result.auc_score:.3f}")
                    print(f"     Training Time: {result.training_time:.1f}s")
            else:
                # Train models for all targets
                print(f"\nðŸ”„ Training models for all targets...")
                all_results = self.training_pipeline.train_all_targets(models)
                
                print(f"\nðŸ“Š Training Summary:")
                for target_col, target_results in all_results.items():
                    if target_results:
                        best_result = max(target_results.values(), key=lambda r: r.test_f1)
                        print(f"   {target_col}:")
                        print(f"     Best Model: {best_result.model_name}")
                        print(f"     Best F1: {best_result.test_f1:.3f}")
                        print(f"     Best Accuracy: {best_result.test_accuracy:.3f}")
                        print(f"     Models Trained: {len(target_results)}")
            
            print(f"\nâœ… Training completed successfully!")
            print(f"ðŸ“ Models saved to: data/models/saved_models/")
            return True
            
        except Exception as e:
            print(f"âŒ Training failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def run_evaluation(self, test_data: str, models_dir: Optional[str] = None, 
                           create_plots: bool = True) -> bool:
        """Run comprehensive model evaluation."""
        print(f"ðŸ“Š Starting Stage 6 Model Evaluation...")
        
        if not IMPLEMENTATIONS_AVAILABLE:
            print("âŒ ML implementations not available.")
            return False
        
        try:
            # Load evaluation configuration
            eval_config = self.load_config("evaluation_config")
            
            # Initialize evaluator
            self.evaluator = ModelEvaluator(output_dir="data/models/evaluation")
            
            # Load test data
            if not Path(test_data).exists():
                print(f"âŒ Test data file not found: {test_data}")
                return False
            
            data = pd.read_csv(test_data)
            print(f"âœ… Loaded {len(data)} test examples")
            
            # Find model files
            models_path = Path(models_dir) if models_dir else Path("data/models/saved_models")
            if not models_path.exists():
                print(f"âŒ Models directory not found: {models_path}")
                return False
            
            model_files = list(models_path.glob("*.joblib"))
            if not model_files:
                print(f"âŒ No model files found in {models_path}")
                print("   Train models first with: python -m src.main train-ml")
                return False
            
            print(f"âœ… Found {len(model_files)} model files")
            
            # Evaluate models for each target
            all_results = []
            targets = ['goal_next_1min_any', 'goal_next_5min_any', 'goal_next_15min_any']
            
            for target in targets:
                if target not in data.columns:
                    print(f"âš ï¸  Target {target} not found in test data")
                    continue
                
                print(f"\nðŸ” Evaluating models for target: {target}")
                target_model_files = [f for f in model_files if target in f.name]
                
                if not target_model_files:
                    print(f"   No models found for {target}")
                    continue
                
                for model_file in target_model_files:
                    try:
                        result = self.evaluator.evaluate_single_model(
                            str(model_file), str(test_data), target
                        )
                        all_results.append(result)
                        print(f"   âœ… {result.model_name}: F1={result.f1:.3f}, Acc={result.accuracy:.3f}, AUC={result.auc:.3f}")
                    except Exception as e:
                        print(f"   âŒ {model_file.name}: {e}")
            
            if not all_results:
                print("âŒ No models successfully evaluated")
                return False
            
            # Generate comprehensive evaluation report
            print(f"\nðŸ“‹ Generating evaluation report...")
            report_path = self.evaluator.generate_evaluation_report(all_results)
            print(f"âœ… Evaluation report: {report_path}")
            
            # Create visualizations
            if create_plots:
                print(f"ðŸ“Š Creating performance visualizations...")
                plots = self.evaluator.create_performance_plots(all_results)
                print(f"âœ… Plots saved to: data/models/evaluation/plots/")
            
            # Export results to CSV
            csv_path = self.evaluator.export_results_to_csv(all_results)
            print(f"âœ… Results exported to: {csv_path}")
            
            # Model comparison summary
            print(f"\nðŸ“ˆ Model Performance Summary:")
            targets_evaluated = list(set(r.target_column for r in all_results))
            for target in targets_evaluated:
                target_results = [r for r in all_results if r.target_column == target]
                if target_results:
                    best = max(target_results, key=lambda r: r.f1)
                    print(f"   {target}:")
                    print(f"     Best Model: {best.model_name} (F1: {best.f1:.3f})")
                    print(f"     Models Evaluated: {len(target_results)}")
            
            return True
                
        except Exception as e:
            print(f"âŒ Evaluation failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def run_prediction(self, match_id: int, minute: int, home_score: int, away_score: int, 
                           home_team_id: int = 1, away_team_id: int = 2) -> bool:
        """Run real-time prediction."""
        print(f"âš½ Making Goal Prediction...")
        print(f"   Match: {match_id}")
        print(f"   Current State: {home_score}-{away_score} at {minute}'")
        
        if not IMPLEMENTATIONS_AVAILABLE:
            print("âŒ ML implementations not available.")
            return False
        
        try:
            # Initialize model manager and prediction engine
            model_manager = ModelManager()
            loaded_models = model_manager.load_best_models()
            
            if not loaded_models:
                print("âŒ No models loaded. Train models first with:")
                print("   python -m src.main train-ml")
                return False
            
            print(f"âœ… Loaded {len(loaded_models)} models")
            
            # Show loaded models by target
            discovered = model_manager.discover_models()
            for target, models in discovered.items():
                model_count = len([m for m in models if any(key.endswith(f"_{target}") for key in loaded_models.keys())])
                if model_count > 0:
                    print(f"   {target}: {model_count} models")
            
            self.prediction_engine = PredictionEngine(model_manager)
            
            # Create match state
            match_state = MatchState(
                match_id=match_id,
                minute=minute,
                home_team_id=home_team_id,
                away_team_id=away_team_id,
                current_score_home=home_score,
                current_score_away=away_score
            )
            
            # Make prediction
            result = self.prediction_engine.predict_all_targets(match_state)
            
            # Display results
            print(f"\nðŸŽ¯ Goal Prediction Results:")
            print(f"   Match {match_id} ({home_score}-{away_score} at {minute}')")
            print(f"   Processing Time: {result.processing_time_ms:.1f}ms")
            print(f"   Overall Confidence: {result.confidence_score:.3f}")
            
            print(f"\nðŸ“Š Ensemble Predictions:")
            for target, probability in result.ensemble_prediction.items():
                timeframe = target.replace('goal_next_', '').replace('_any', '')
                print(f"   Next {timeframe}: {probability:.1%} chance")
            
            print(f"\nðŸ¤– Individual Model Predictions:")
            for target, target_predictions in result.predictions.items():
                if target_predictions:
                    timeframe = target.replace('goal_next_', '').replace('_any', '')
                    print(f"   {timeframe}:")
                    for model_key, pred in target_predictions.items():
                        if 'error' not in pred:
                            model_name = model_key.split('_')[0]
                            print(f"     {model_name}: {pred['probability']:.1%} (conf: {pred['confidence']:.2f})")
            
            return True
            
        except Exception as e:
            print(f"âŒ Prediction failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def run_prediction_demo(self) -> bool:
        """Run prediction demo with multiple scenarios."""
        print(f"ðŸŽ® Running Goal Prediction Demo...")
        
        scenarios = [
            {
                "name": "Late Game Tie", 
                "match_id": 12345, "minute": 85, "home_score": 1, "away_score": 1,
                "description": "Tight match in final minutes"
            },
            {
                "name": "Losing Team Pressure",
                "match_id": 12346, "minute": 70, "home_score": 0, "away_score": 1, 
                "description": "Home team needs to equalize"
            },
            {
                "name": "Comfortable Lead",
                "match_id": 12347, "minute": 75, "home_score": 2, "away_score": 0,
                "description": "Home team protecting lead"
            }
        ]
        
        success_count = 0
        for i, scenario in enumerate(scenarios, 1):
            print(f"\nðŸ“Š Scenario {i}: {scenario['name']}")
            print(f"   {scenario['description']}")
            
            success = await self.run_prediction(
                scenario['match_id'], scenario['minute'], 
                scenario['home_score'], scenario['away_score']
            )
            
            if success:
                success_count += 1
        
        print(f"\nâœ… Demo completed: {success_count}/{len(scenarios)} scenarios successful")
        return success_count == len(scenarios)
    
    async def run_api_server(self, host: str = "0.0.0.0", port: int = 8001) -> bool:
        """Start the prediction API server."""
        print(f"ðŸš€ Starting ML Prediction API Server...")
        print(f"   Host: {host}")
        print(f"   Port: {port}")
        
        if not IMPLEMENTATIONS_AVAILABLE:
            print("âŒ ML implementations not available.")
            return False
        
        try:
            # Load API configuration
            api_config = self.load_config("prediction_config")
            
            # Override with command line parameters
            if api_config.get("api"):
                api_config["api"]["host"] = host
                api_config["api"]["port"] = port
            
            print(f"âœ… Configuration loaded")
            print(f"ðŸ“š API Documentation: http://{host}:{port}/docs")
            print(f"ðŸ”„ Health Check: http://{host}:{port}/health")
            
            # Start server
            await run_api_server(host=host, port=port)
            return True
            
        except Exception as e:
            print(f"âŒ API server failed: {e}")
            import traceback
            traceback.print_exc()
            return False

# Command functions for main.py integration
async def run_ml_training(training_data: str, target: Optional[str] = None, 
                         models: Optional[List[str]] = None) -> bool:
    """Train ML models."""
    runner = MLCommandRunner()
    return await runner.run_training(training_data, target, models)

async def run_ml_evaluation(test_data: str, models_dir: Optional[str] = None, 
                           create_plots: bool = True) -> bool:
    """Evaluate ML models."""
    runner = MLCommandRunner()
    return await runner.run_evaluation(test_data, models_dir, create_plots)

async def run_ml_prediction(match_id: int, minute: int, home_score: int, away_score: int, 
                           home_team_id: int = 1, away_team_id: int = 2) -> bool:
    """Make goal prediction."""
    runner = MLCommandRunner()
    return await runner.run_prediction(match_id, minute, home_score, away_score, home_team_id, away_team_id)

async def run_ml_demo() -> bool:
    """Run prediction demo."""
    runner = MLCommandRunner()
    return await runner.run_prediction_demo()

async def run_ml_api_server(host: str = "0.0.0.0", port: int = 8001) -> bool:
    """Start API server."""
    runner = MLCommandRunner()
    return await runner.run_api_server(host, port)

# Standalone testing
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python cli_commands.py [train|evaluate|predict|demo|serve]")
        sys.exit(1)
    
    command = sys.argv[1]
    
    if command == "train":
        asyncio.run(run_ml_training("demo_training_dataset.csv"))
    elif command == "evaluate":
        asyncio.run(run_ml_evaluation("demo_training_dataset.csv"))
    elif command == "predict":
        asyncio.run(run_ml_prediction(12345, 75, 1, 0))
    elif command == "demo":
        asyncio.run(run_ml_demo())
    elif command == "serve":
        asyncio.run(run_ml_api_server())
    else:
        print(f"Unknown command: {command}")
EOF

echo "âœ… CLI commands created"

# Create enhanced main.py with Stage 6 integration
cat > src/main_ml_integration.py << 'EOF'
"""
Enhanced main.py with Stage 6 ML Model Development integration.
Add these commands to your existing src/main.py file.
"""

# Add these imports to your existing main.py
try:
    from src.ml_models.cli_commands import (
        run_ml_training, run_ml_evaluation, run_ml_prediction,
        run_ml_demo, run_ml_api_server
    )
    ML_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ML models not available: {e}")
    ML_AVAILABLE = False

# Add these CLI commands to your existing click group in main.py

@cli.command()
@click.option('--data', '-d', default='demo_training_dataset.csv', help='Path to training data CSV file')
@click.option('--target', '-t', help='Specific target column to train')
@click.option('--models', '-m', multiple=True, help='Specific models to train (xgboost, random_forest, logistic_regression)')
def train_ml(data, target, models):
    """Train ML models for goal prediction."""
    if not ML_AVAILABLE:
        click.echo("âŒ ML models not available. Install requirements:")
        click.echo("   pip install -r requirements_ml.txt")
        return
    
    models_list = list(models) if models else None
    success = asyncio.run(run_ml_training(data, target, models_list))
    
    if success:
        click.echo("âœ… ML training completed successfully!")
    else:
        click.echo("âŒ ML training failed. Check logs for details.")

@cli.command()
@click.option('--data', '-d', default='demo_training_dataset.csv', help='Path to test data CSV file')
@click.option('--models-dir', '-m', help='Directory containing saved models')
@click.option('--no-plots', is_flag=True, help='Skip plot generation')
def evaluate_ml(data, models_dir, no_plots):
    """Evaluate trained ML models."""
    if not ML_AVAILABLE:
        click.echo("âŒ ML models not available.")
        return
    
    success = asyncio.run(run_ml_evaluation(data, models_dir, not no_plots))
    
    if success:
        click.echo("âœ… ML evaluation completed successfully!")
    else:
        click.echo("âŒ ML evaluation failed. Check logs for details.")

@cli.command()
@click.option('--match-id', '-i', type=int, required=True, help='Match ID')
@click.option('--minute', '-min', type=int, required=True, help='Current minute')
@click.option('--home-score', '-hs', type=int, required=True, help='Home team score')
@click.option('--away-score', '-as', type=int, required=True, help='Away team score')
@click.option('--home-team-id', type=int, default=1, help='Home team ID')
@click.option('--away-team-id', type=int, default=2, help='Away team ID')
def predict_ml(match_id, minute, home_score, away_score, home_team_id, away_team_id):
    """Make goal prediction for current match state."""
    if not ML_AVAILABLE:
        click.echo("âŒ ML models not available.")
        return
    
    success = asyncio.run(run_ml_prediction(
        match_id, minute, home_score, away_score, home_team_id, away_team_id
    ))
    
    if not success:
        click.echo("âŒ Prediction failed. Check logs for details.")

@cli.command()
def predict_ml_demo():
    """Run goal prediction demo with sample scenarios."""
    if not ML_AVAILABLE:
        click.echo("âŒ ML models not available.")
        return
    
    success = asyncio.run(run_ml_demo())
    
    if success:
        click.echo("âœ… ML demo completed successfully!")
    else:
        click.echo("âŒ ML demo failed. Check logs for details.")

@cli.command()
@click.option('--host', '-h', default='0.0.0.0', help='API server host')
@click.option('--port', '-p', type=int, default=8001, help='API server port')
def serve_ml(host, port):
    """Start the prediction API server."""
    if not ML_AVAILABLE:
        click.echo("âŒ ML models not available.")
        return
    
    click.echo(f"ðŸš€ Starting ML API server on {host}:{port}")
    click.echo(f"ðŸ“š Documentation: http://{host}:{port}/docs")
    
    try:
        asyncio.run(run_ml_api_server(host, port))
    except KeyboardInterrupt:
        click.echo("\nðŸ‘‹ API server stopped")

@cli.command()
def ml_status():
    """Show ML pipeline status and information."""
    click.echo("ðŸ“Š Stage 6 ML Pipeline Status")
    click.echo("=" * 40)
    
    # Check training data
    training_data_exists = Path("demo_training_dataset.csv").exists()
    click.echo(f"Training Data: {'âœ…' if training_data_exists else 'âŒ'} demo_training_dataset.csv")
    
    if training_data_exists:
        try:
            import pandas as pd
            df = pd.read_csv("demo_training_dataset.csv")
            click.echo(f"  Samples: {len(df)}")
            click.echo(f"  Features: {len(df.columns)}")
            
            # Check target distributions
            targets = ['goal_next_1min_any', 'goal_next_5min_any', 'goal_next_15min_any']
            for target in targets:
                if target in df.columns:
                    positive = df[target].sum()
                    total = len(df)
                    click.echo(f"  {target}: {positive}/{total} ({positive/total*100:.1f}%)")
        except Exception as e:
            click.echo(f"  Error reading data: {e}")
    
    # Check models directory
    models_dir = Path("data/models/saved_models")
    models_exist = models_dir.exists()
    click.echo(f"Models Directory: {'âœ…' if models_exist else 'âŒ'} {models_dir}")
    
    if models_exist:
        model_files = list(models_dir.glob("*.joblib"))
        click.echo(f"  Saved Models: {len(model_files)}")
        
        # Group by target
        targets_found = set()
        for model_file in model_files:
            for target in ['goal_next_1min_any', 'goal_next_5min_any', 'goal_next_15min_any']:
                if target in model_file.name:
                    targets_found.add(target)
        
        for target in targets_found:
            target_models = [f for f in model_files if target in f.name]
            click.echo(f"  {target}: {len(target_models)} models")
    
    # Check ML availability
    click.echo(f"ML Available: {'âœ…' if ML_AVAILABLE else 'âŒ'}")
    
    if not ML_AVAILABLE:
        click.echo("To enable ML features:")
        click.echo("  pip install -r requirements_ml.txt")
    
    # Check configuration
    config_dir = Path("config/ml_models")
    config_exists = config_dir.exists()
    click.echo(f"Configuration: {'âœ…' if config_exists else 'âŒ'} {config_dir}")
    
    if config_exists:
        config_files = list(config_dir.glob("*.json"))
        click.echo(f"  Config Files: {len(config_files)}")
    
    # Usage examples
    if training_data_exists and ML_AVAILABLE:
        click.echo("\nðŸŽ¯ Quick Start Commands:")
        click.echo("  python -m src.main train-ml")
        click.echo("  python -m src.main evaluate-ml") 
        click.echo("  python -m src.main predict-ml --match-id 12345 --minute 75 --home-score 1 --away-score 0")
        click.echo("  python -m src.main serve-ml")
EOF

echo "âœ… Main.py integration code created"

# Create test files
cat > tests/ml_models/test_training.py << 'EOF'
"""
Tests for ML training pipeline.
"""

import pytest
import pandas as pd
import numpy as np
from pathlib import Path
import tempfile
import os

from src.ml_models.training.pipeline import MLTrainingPipeline, TrainingConfig


@pytest.fixture
def sample_training_data():
    """Create sample training data for testing."""
    np.random.seed(42)
    n_samples = 100
    
    data = {
        'match_id': np.random.randint(10000, 99999, n_samples),
        'minute': np.random.randint(59, 91, n_samples),
        'home_team_id': np.random.randint(1, 20, n_samples),
        'away_team_id': np.random.randint(1, 20, n_samples),
        'current_score_home': np.random.randint(0, 4, n_samples),
        'current_score_away': np.random.randint(0, 4, n_samples),
        'score_difference': np.random.randint(-3, 4, n_samples),
        'total_goals': np.random.randint(0, 6, n_samples),
        'is_late_game': np.random.choice([0, 1], n_samples),
        'is_extra_time': np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),
        'goal_next_1min_any': np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),
        'goal_next_5min_any': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),
        'goal_next_15min_any': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),
        'feature_generated_at': ['2025-07-04T10:00:00'] * n_samples
    }
    
    return pd.DataFrame(data)


@pytest.fixture
def temp_training_file(sample_training_data):
    """Create temporary training data file."""
    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
        sample_training_data.to_csv(f.name, index=False)
        yield f.name
    os.unlink(f.name)


@pytest.fixture
def training_config():
    """Create test training configuration."""
    return TrainingConfig(
        test_size=0.3,
        cv_folds=3,
        enable_hyperparameter_tuning=False,  # Disable for faster testing
        model_save_path=tempfile.mkdtemp()
    )


class TestMLTrainingPipeline:
    """Test ML training pipeline functionality."""
    
    def test_initialization(self, training_config):
        """Test pipeline initialization."""
        pipeline = MLTrainingPipeline(training_config)
        assert pipeline.config == training_config
        assert len(pipeline.target_columns) == 3
        assert 'xgboost' in pipeline.model_definitions or True  # May not be available
    
    def test_load_training_data(self, temp_training_file, training_config):
        """Test training data loading."""
        pipeline = MLTrainingPipeline(training_config)
        data = pipeline.load_training_data(temp_training_file)
        
        assert len(data) == 100
        assert 'goal_next_1min_any' in data.columns
        assert 'goal_next_5min_any' in data.columns
        assert 'goal_next_15min_any' in data.columns
    
    def test_prepare_features(self, temp_training_file, training_config):
        """Test feature preparation."""
        pipeline = MLTrainingPipeline(training_config)
        data = pipeline.load_training_data(temp_training_file)
        
        X, y, feature_names = pipeline.prepare_features('goal_next_5min_any')
        
        assert X.shape[0] == len(data)
        assert len(y) == len(data)
        assert len(feature_names) > 0
        assert 'match_id' not in feature_names  # Should be excluded
        assert 'goal_next_5min_any' not in feature_names  # Should be excluded
    
    def test_feature_selection(self, temp_training_file, training_config):
        """Test feature selection."""
        pipeline = MLTrainingPipeline(training_config)
        data = pipeline.load_training_data(temp_training_file)
        X, y, feature_names = pipeline.prepare_features('goal_next_5min_any')
        
        X_selected, selector, selected_features = pipeline.apply_feature_selection(X, y, feature_names)
        
        if training_config.enable_feature_selection:
            assert X_selected.shape[1] <= X.shape[1]
            assert len(selected_features) <= len(feature_names)
        else:
            assert X_selected.shape[1] == X.shape[1]
    
    def test_scaling(self, temp_training_file, training_config):
        """Test feature scaling."""
        pipeline = MLTrainingPipeline(training_config)
        data = pipeline.load_training_data(temp_training_file)
        X, y, feature_names = pipeline.prepare_features('goal_next_5min_any')
        
        # Split data
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        
        X_train_scaled, X_test_scaled, scaler = pipeline.apply_scaling(X_train, X_test)
        
        if training_config.enable_scaling:
            # Check that scaling was applied (mean close to 0, std close to 1)
            assert abs(np.mean(X_train_scaled)) < 1.0
            assert scaler is not None
        else:
            assert np.array_equal(X_train_scaled, X_train)
    
    @pytest.mark.slow
    def test_train_single_model(self, temp_training_file, training_config):
        """Test training a single model."""
        pipeline = MLTrainingPipeline(training_config)
        data = pipeline.load_training_data(temp_training_file)
        
        # Test with logistic regression (should be available)
        result = pipeline.train_single_model('logistic_regression', 'goal_next_5min_any')
        
        assert result.model_name == 'logistic_regression'
        assert result.target_column == 'goal_next_5min_any'
        assert result.model is not None
        assert 0 <= result.test_accuracy <= 1
        assert 0 <= result.test_f1 <= 1
        assert result.training_time > 0


@pytest.mark.integration
class TestMLTrainingIntegration:
    """Integration tests for ML training pipeline."""
    
    def test_full_training_pipeline(self, temp_training_file):
        """Test complete training pipeline."""
        config = TrainingConfig(
            test_size=0.3,
            cv_folds=3,
            enable_hyperparameter_tuning=False,
            model_save_path=tempfile.mkdtemp()
        )
        
        pipeline = MLTrainingPipeline(config)
        data = pipeline.load_training_data(temp_training_file)
        
        # Train models for one target
        results = pipeline.train_all_models('goal_next_5min_any')
        
        assert len(results) > 0
        assert 'logistic_regression' in results
        
        for model_name, result in results.items():
            assert result.model is not None
            assert result.test_accuracy >= 0
            assert result.test_f1 >= 0
EOF

echo "âœ… Training tests created"

# Create a simple demo script
cat > demo_stage6_ml.py << 'EOF'
#!/usr/bin/env python3
"""
Stage 6 ML Model Development - Complete Demo Script

This script demonstrates the complete ML pipeline for football goal prediction.
Run this to test all Stage 6 functionality.
"""

import asyncio
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

async def run_complete_ml_demo():
    """Run complete ML pipeline demonstration."""
    print("ðŸš€ Stage 6 ML Model Development - Complete Demo")
    print("=" * 60)
    
    # Check prerequisites
    print("\n1ï¸âƒ£ Checking Prerequisites...")
    
    # Check training data
    training_data = Path("demo_training_dataset.csv")
    if not training_data.exists():
        print("âŒ Training data not found: demo_training_dataset.csv")
        print("   Run Stage 5 feature engineering first:")
        print("   python src/feature_engineering/cli_commands.py")
        return False
    
    try:
        import pandas as pd
        df = pd.read_csv(training_data)
        print(f"âœ… Training data: {len(df)} samples, {len(df.columns)} features")
    except ImportError:
        print("âŒ pandas not available. Install with: pip install pandas")
        return False
    
    # Check ML dependencies
    try:
        from src.ml_models.cli_commands import MLCommandRunner
        print("âœ… ML modules available")
    except ImportError as e:
        print(f"âŒ ML modules not available: {e}")
        print("   Install requirements: pip install -r requirements_ml.txt")
        return False
    
    # Initialize ML runner
    runner = MLCommandRunner()
    
    # 2. Training Phase
    print("\n2ï¸âƒ£ Training ML Models...")
    training_success = await runner.run_training("demo_training_dataset.csv")
    
    if not training_success:
        print("âŒ Training failed. Cannot continue demo.")
        return False
    
    # 3. Evaluation Phase
    print("\n3ï¸âƒ£ Evaluating Models...")
    evaluation_success = await runner.run_evaluation("demo_training_dataset.csv")
    
    if not evaluation_success:
        print("âš ï¸  Evaluation failed, but continuing demo...")
    
    # 4. Prediction Demo
    print("\n4ï¸âƒ£ Running Prediction Demo...")
    prediction_success = await runner.run_prediction_demo()
    
    if not prediction_success:
        print("âš ï¸  Prediction demo had issues, but continuing...")
    
    # 5. Summary
    print("\n5ï¸âƒ£ Demo Summary")
    print("=" * 30)
    print(f"Training: {'âœ…' if training_success else 'âŒ'}")
    print(f"Evaluation: {'âœ…' if evaluation_success else 'âŒ'}")
    print(f"Prediction: {'âœ…' if prediction_success else 'âŒ'}")
    
    # Show file locations
    print("\nðŸ“ Generated Files:")
    
    models_dir = Path("data/models/saved_models")
    if models_dir.exists():
        model_files = list(models_dir.glob("*.joblib"))
        print(f"   Models: {len(model_files)} files in {models_dir}")
    
    eval_dir = Path("data/models/evaluation")
    if eval_dir.exists():
        reports = list(eval_dir.glob("**/*.json"))
        plots = list(eval_dir.glob("**/*.png"))
        print(f"   Evaluation: {len(reports)} reports, {len(plots)} plots in {eval_dir}")
    
    # Next steps
    print("\nðŸŽ¯ Next Steps:")
    print("   1. Try individual predictions:")
    print("      python -m src.main predict-ml --match-id 12345 --minute 80 --home-score 2 --away-score 1")
    print("   2. Start prediction API:")
    print("      python -m src.main serve-ml")
    print("   3. Check model performance:")
    print("      python -m src.main ml-status")
    
    return training_success and evaluation_success and prediction_success

if __name__ == "__main__":
    success = asyncio.run(run_complete_ml_demo())
    if success:
        print("\nðŸŽ‰ Stage 6 ML Demo completed successfully!")
    else:
        print("\nâš ï¸  Stage 6 ML Demo completed with issues. Check logs above.")
    
    sys.exit(0 if success else 1)
EOF

chmod +x demo_stage6_ml.py

echo "âœ… Demo script created"

# Create final validation script
cat > validate_stage6_setup.py << 'EOF'
#!/usr/bin/env python3
"""
Stage 6 Setup Validation Script

Validates that all Stage 6 components are properly installed and configured.
"""

import sys
from pathlib import Path
import importlib

def check_file_exists(file_path, description):
    """Check if a file exists and report status."""
    path = Path(file_path)
    exists = path.exists()
    status = "âœ…" if exists else "âŒ"
    print(f"{status} {description}: {file_path}")
    if exists and path.is_file():
        size = path.stat().st_size
        print(f"    Size: {size:,} bytes")
    return exists

def check_directory_exists(dir_path, description):
    """Check if a directory exists and report contents."""
    path = Path(dir_path)
    exists = path.exists()
    status = "âœ…" if exists else "âŒ"
    print(f"{status} {description}: {dir_path}")
    if exists and path.is_dir():
        files = list(path.iterdir())
        print(f"    Contents: {len(files)} items")
    return exists

def check_python_import(module_name, description):
    """Check if a Python module can be imported."""
    try:
        importlib.import_module(module_name)
        print(f"âœ… {description}: {module_name}")
        return True
    except ImportError as e:
        print(f"âŒ {description}: {module_name} - {e}")
        return False

def main():
    """Run complete Stage 6 validation."""
    print("ðŸ” Stage 6 ML Model Development - Setup Validation")
    print("=" * 60)
    
    all_checks_passed = True
    
    # 1. Check directory structure
    print("\nðŸ“ Directory Structure:")
    directories = [
        ("src/ml_models", "ML Models Package"),
        ("src/ml_models/training", "Training Pipeline"),
        ("src/ml_models/evaluation", "Evaluation Framework"),
        ("src/ml_models/prediction", "Prediction API"),
        ("config/ml_models", "ML Configuration"),
        ("data/models", "Models Storage"),
        ("data/models/saved_models", "Saved Models"),
        ("data/models/evaluation", "Evaluation Results"),
        ("tests/ml_models", "ML Tests")
    ]
    
    for dir_path, description in directories:
        if not check_directory_exists(dir_path, description):
            all_checks_passed = False
    
    # 2. Check core files
    print("\nðŸ“„ Core Files:")
    files = [
        ("src/ml_models/__init__.py", "ML Package Init"),
        ("src/ml_models/cli_commands.py", "CLI Commands"),
        ("config/ml_models/training_config.json", "Training Config"),
        ("config/ml_models/evaluation_config.json", "Evaluation Config"),
        ("config/ml_models/prediction_config.json", "Prediction Config"),
        ("requirements_ml.txt", "ML Requirements"),
        ("demo_stage6_ml.py", "Demo Script"),
        ("validate_stage6_setup.py", "Validation Script")
    ]
    
    for file_path, description in files:
        if not check_file_exists(file_path, description):
            all_checks_passed = False
    
    # 3. Check training data
    print("\nðŸ“Š Training Data:")
    data_files = [
        ("demo_training_dataset.csv", "Main Training Dataset"),
        ("demo_features/match_12345_features.json", "Sample Features")
    ]
    
    for file_path, description in data_files:
        if not check_file_exists(file_path, description):
            all_checks_passed = False
    
    # Check training data content
    if Path("demo_training_dataset.csv").exists():
        try:
            import pandas as pd
            df = pd.read_csv("demo_training_dataset.csv")
            print(f"    Samples: {len(df)}")
            print(f"    Features: {len(df.columns)}")
            
            # Check target columns
            targets = ['goal_next_1min_any', 'goal_next_5min_any', 'goal_next_15min_any']
            for target in targets:
                if target in df.columns:
                    positive_count = df[target].sum()
                    total_count = len(df)
                    print(f"    {target}: {positive_count}/{total_count} ({positive_count/total_count*100:.1f}% positive)")
                else:
                    print(f"    âŒ Missing target: {target}")
                    all_checks_passed = False
        except ImportError:
            print("    âš ï¸  Cannot validate data content (pandas not available)")
        except Exception as e:
            print(f"    âŒ Error reading training data: {e}")
            all_checks_passed = False
    
    # 4. Check Python dependencies
    print("\nðŸ Python Dependencies:")
    dependencies = [
        ("pandas", "Data Processing"),
        ("numpy", "Numerical Computing"),
        ("scikit-learn", "Machine Learning"),
        ("matplotlib", "Plotting"),
        ("seaborn", "Statistical Visualization"),
        ("joblib", "Model Persistence"),
        ("click", "CLI Framework")
    ]
    
    for module, description in dependencies:
        if not check_python_import(module, description):
            all_checks_passed = False
    
    # Optional dependencies
    print("\nðŸ“¦ Optional Dependencies:")
    optional_deps = [
        ("xgboost", "XGBoost Models"),
        ("fastapi", "API Framework"),
        ("uvicorn", "API Server"),
        ("plotly", "Interactive Plots"),
        ("optuna", "Hyperparameter Optimization")
    ]
    
    for module, description in optional_deps:
        check_python_import(module, description)
    
    # 5. Check ML module imports
    print("\nðŸ¤– ML Module Imports:")
    try:
        from src.ml_models.cli_commands import MLCommandRunner
        print("âœ… CLI Commands: MLCommandRunner")
    except ImportError as e:
        print(f"âŒ CLI Commands: {e}")
        all_checks_passed = False
    
    # 6. Configuration validation
    print("\nâš™ï¸  Configuration Validation:")
    config_files = [
        "config/ml_models/training_config.json",
        "config/ml_models/evaluation_config.json", 
        "config/ml_models/prediction_config.json"
    ]
    
    for config_file in config_files:
        try:
            import json
            with open(config_file) as f:
                config = json.load(f)
            print(f"âœ… {Path(config_file).name}: Valid JSON")
        except FileNotFoundError:
            print(f"âŒ {Path(config_file).name}: File not found")
            all_checks_passed = False
        except json.JSONDecodeError as e:
            print(f"âŒ {Path(config_file).name}: Invalid JSON - {e}")
            all_checks_passed = False
        except Exception as e:
            print(f"âŒ {Path(config_file).name}: Error - {e}")
            all_checks_passed = False
    
    # 7. Final summary
    print("\nðŸ“‹ Validation Summary:")
    print("=" * 30)
    
    if all_checks_passed:
        print("ðŸŽ‰ All checks passed! Stage 6 is ready for use.")
        print("\nðŸš€ Quick Start Commands:")
        print("   python demo_stage6_ml.py")
        print("   python -m src.main train-ml")
        print("   python -m src.main ml-status")
        return True
    else:
        print("âŒ Some checks failed. Please address the issues above.")
        print("\nðŸ”§ Common fixes:")
        print("   pip install -r requirements_ml.txt")
        print("   python src/feature_engineering/cli_commands.py  # Generate training data")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
EOF

chmod +x validate_stage6_setup.py

echo ""
echo "âœ… Stage 6 ML Model Development Setup Complete!"
echo ""
echo "ðŸ“ Created Structure:"
echo "   src/ml_models/          - Complete ML pipeline package"
echo "   config/ml_models/       - ML configuration files"
echo "   data/models/            - Model storage directories"
echo "   requirements_ml.txt     - ML dependencies"
echo "   demo_stage6_ml.py       - Complete demo script"
echo ""
echo "ðŸ”„ Next Steps:"
echo "   1. Install ML dependencies:"
echo "      pip install -r requirements_ml.txt"
echo ""
echo "   2. Validate setup:"
echo "      python validate_stage6_setup.py"
echo ""
echo "   3. Run complete demo:"
echo "      python demo_stage6_ml.py"
echo ""
echo "   4. Or use individual commands:"
echo "      python -m src.main train-ml"
echo "      python -m src.main evaluate-ml"
echo "      python -m src.main predict-ml --match-id 12345 --minute 75 --home-score 1 --away-score 0"
echo "      python -m src.main serve-ml"
echo ""
echo "ðŸŽ¯ Stage 6 ML Model Development is ready for football goal prediction!"